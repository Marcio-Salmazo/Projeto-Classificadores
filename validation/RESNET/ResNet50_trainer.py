import tensorflow as tf
import os


class Trainer:
    """
        Classe respons√°vel por treinar o modelo ResNet-50 criado.
        Ela n√£o cria o modelo nem o loader; apenas recebe ambos.
    """

    def __init__(self, model, train_ds, val_ds, epochs=90, initial_lr=0.1, momentum=0.9, weight_decay=1e-4, patience=3,
                 log_dir="logs", checkpoint_path="checkpoints/resnet50_best.h5"
                 ):
        self.model = model
        self.train_ds = train_ds
        self.val_ds = val_ds
        self.epochs = epochs
        self.initial_lr = initial_lr
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.patience = patience
        self.log_dir = log_dir
        self.checkpoint_path = checkpoint_path

        # Garantir que diret√≥rios para armazenamento de pesos e logs existam
        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)

        # Otimizador definido igual ao artigo (SGD - Stochastic Gradient Descent + momentum)
        self.optimizer = tf.keras.optimizers.SGD(
            learning_rate=initial_lr,
            momentum=momentum
        )

        # Compila o modelo logo na inst√¢ncia da classe
        self.compile_model()
        # Configura callbacks logo na inst√¢ncia da classe
        self.callbacks = self.create_callbacks()

    # ------------------------------------------------------------------------------------------------------------------
    def compile_model(self):
        """
        Compila o modelo conforme os par√¢metros definidos no artigo
            - Loss: SparseCategoricalCrossentropy
                 * √â usada quando h√° um problema em que cada exemplo pertence a
                   exatamente uma de v√°rias classes poss√≠veis
                 * O termo "Sparse" (esparso) refere-se especificamente ao formato
                   em que os r√≥tulos de treinamento s√£o fornecidos. Em vez de usar a codifica√ß√£o
                   one-hot, os r√≥tulos s√£o fornecidos como inteiros √∫nicos (ImageNet usa labels inteiros).
            - M√©tricas: top-1 accuracy e top-5 accuracy
        """
        self.model.compile(
            optimizer=self.optimizer,
            loss=tf.keras.losses.SparseCategoricalCrossentropy(),
            metrics=[
                "accuracy",
                tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name="top5")
            ]
        )

    def lr_scheduler(self):
        """
        Learning rate decay por plateau ‚Äî fiel ao artigo:
        - Come√ßa em 0.1
        - Divide por 10 quando a m√©trica de avalia√ß√£o (val_top5) estaciona
        """
        return tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_top5",  # m√©trica usada no paper (erro/top-5)
            factor=0.1,  # divide LR por 10
            patience=self.patience,  # aguarda 3 √©pocas sem melhora
            mode="max",
            verbose=1,
            min_lr=1e-5  # LR m√≠nimo (opcional)
        )

    '''
    # O paper n√£o define exatamente quando considerar que ocorreu plateau,
    # O reposit√≥rio oficial em Caffe mostra que plateaus ocorrem APROXIMADAMENTE 
    # em pontos que correspondem √†s epochs 30 e 60, Frameworks modernos adotaram isso como ‚Äúregra‚Äù.
    
    def lr_scheduler(self):
        """
        Step decay learning rate schedule igual ao paper:

        O paper usa LR inicial 0.1 e reduz por 10√ó nas √©pocas 30 e 60,
        treinando por 90 √©pocas.
        """

        def schedule(epoch, lr):
            if epoch < 30:
                return self.initial_lr
            elif epoch < 60:
                return self.initial_lr * 0.1
            else:
                return self.initial_lr * 0.01

        return tf.keras.callbacks.LearningRateScheduler(schedule, verbose=1)
    '''

    def create_callbacks(self):
        """
            Cria todos os callbacks usados no treinamento.
        """
        return [
            self.lr_scheduler(),
            # Salva o melhor modelo baseado em val_top5
            tf.keras.callbacks.ModelCheckpoint(
                filepath=self.checkpoint_path,
                monitor="val_top5",
                mode="max",
                # Salva apenas quando a m√©trica melhora
                save_best_only=True,
                # Salva o modelo completo (arquitetura + pesos).
                save_weights_only=False,
                verbose=1
            ),
            # Registra logs para visualiza√ß√£o (perda, m√©tricas, histogramas).
            tf.keras.callbacks.TensorBoard(log_dir=self.log_dir),

            # üîµ CSV Logger ‚Äî salva m√©tricas a cada epoch
            tf.keras.callbacks.CSVLogger(
                filename="training_metrics.csv",
                separator=",",
                append=False
            )
        ]

    def train(self):
        """
            Executa o processo completo de treinamento.
        """
        print("\nIniciando treinamento da ResNet-50...\n")

        history = self.model.fit(
            self.train_ds,
            epochs=self.epochs,
            validation_data=self.val_ds,
            callbacks=self.callbacks
        )

        print("\nTreinamento finalizado!")
        print(f"Melhor modelo salvo em: {self.checkpoint_path}\n")

        return history
